GRADING SCHEME: Answer ALL regular questions correctly OR TWO BONUS question to pass.



Question 1

Question 2

Question 3

Question 4

Question 5

Question 6

Question 7



Question 1
----------
Why could missing data ("NaN") be problematic for machine learning models?
- Do we have good non-trivial reasons ("Most machine learning algorithms cannot work with missing features") here...??



Bonus Exercise:
---------------
Choose another feature and create a similar bar graph in the code cell below. Save your graph and describe any trends you observe: Does the selected feature predict survival?
- [For grading / Example solution: give a few examples here]



Question X
----------
Which are the top 3 feature pairs that are most strongly (positively or negatively) correlated? Try to explain one of these relationships.
- List top 3
- List explanation examples for all three



Question X
----------
For each of the features `PassengerId`, `Name`, and `Ticket`, can you think of a reason why they may or may not contribute to our model?
- PassengerId: Random information, irrelevant
- Name: reflects Sex (which we keep); possible title > status: likely reflected in Pclass and Fare; (( in addition, might also contain errors as are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.))
- Ticket number: somewhat random, likely reflected in Fare, possibly in SibSp or Parch??

If you were to delete a further feature from the data set, which one would you delete and why?
- Embarked: possibly reflected in Pclass; but might be worth keeping if related to the position of the cabins
- Parch / SibSp: could be removed, ideally combined into a new feature
- #SP: other ideas?


Bonus Question: 
--------------
In general, what could be good reasons to confine ourselves to the most relevant features in the data?
- Reduce risk of overfitting! (less free, meaningless parameters increase the risk of fitting specifities in the training data and not generalising / not capturing true patterns in the data
- Further: less paramaters improve transparency and interpretability of the results!


Bonus Question:    
--------------
One issue with replacing the port designations S, C, and Q by 0, 1, and 2 is that machine learning algorithms will assume that *two nearby values are more similar than two distant values*. This may be fine in some cases (e.g. for ordered categories such as "bad", "average", "good", and "excellent"), but it is obviously not the case for the `Embarked` column. Can you come up with an alternative replacement scheme to avoid this problem?

- Explain one-hot encoding: create "dummy" attributes (or columns) so that one attribute will be equal to 1 (hot) while the others will be 0 (cold).




Question X
----------
What could be better ways to replace the many missing age values than just using one single value (the median) for all of them?
- Use two different medians for male and female passengers
- Draw random numbers based on the distribution (mean and std) of the given ages
- (Advanced) Create a separate category of passengers with missing ages (as a separate "age band")

Question X
----------
Why do you think is it reasonable to split the `Age` and `Fare` features into groups?</div>
- ...


