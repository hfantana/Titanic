{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster\n",
    "\n",
    "##### Based on the famous [kaggle Titanic competition](https://www.kaggle.com/c/titanic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal: Work through a simple machine learning example from start to finish along a typical data analysis pipeline. \n",
    "\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered \"unsinkable\" RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren't enough lifeboats for everyone on board, resulting in the death of 1514 of the 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some were more likely to survive than others. We will use machine learning to create a model that predicts which passengers survived the Titanic shipwreck. \n",
    "\n",
    "<div align=\"center\">\n",
    "<img width=\"1200\" title=\"Titanic at Southampton docks, prior to departure\" src=\"images/titanic.jpg\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note**: This exercise is based on a Jupyter notebook, an interactive environment for writing and running code, and is running in Python. To get familiar with working in Jupyter notebooks, see our short \"JupyterLab Tutorial\". For a basic introduction to programming in Python, see the \"Introduction to Python\" notebook.\n",
    "\n",
    "<mark>We should remember to add the Jupyter Tutorial and Intro to Python notebook (as optional!) when assembling a skill </mark>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">A cell like this indicates a question you need to answer in the Answer.txt file. Please answer the question <b>before</b> continuing through the notebook. You can <b>double click on Answer.txt</b> in the Left Sidebar now to open it in a new tab. As you go through the notebook, navigate between the tabs to answer questions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#1.-Introduction)\n",
    "\n",
    "2. [Get familiar with the data](#2.-Get-familiar-with-the-data)\n",
    "\n",
    "3. [Further explore the data](#3.-Further-explore-the-data)\n",
    "\n",
    "4. [Prepare the data](#4.-Prepare-the-data)\n",
    "\n",
    "   1. [Remove less relevant features](#4A.-Remove-less-relevant-features)\n",
    "   2. [Convert text-based features](#4B.-Convert-text-based-features)\n",
    "   3. [Fill in missing data](#4C.-Fill-in-missing-data)\n",
    "   4. [Derive new features](#4D.-Derive-new-features)\n",
    "   \n",
    "   \n",
    "5. [Train and evaluate a model](#5.-Train-and-evaluate-a-model)\n",
    "\n",
    "6. [Sources](#Sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Challenge, we will build a predictive model to answer the question **\"Who was more likely to survive on the Titanic?\"** using passenger data. These data contain information about each passenger (e.g., the passenger's name, age, gender, ticket class, etc.). For this exercise, we will use a subset of the full passenger data set (891 of the 2224 passengers and crew on board). Importantly, our data set `titanic.csv` contains also the information whether each passenger survived or not, i.e. the *label* required for using supervised machine learning methods.\n",
    "\n",
    "To build our model we will work through a number of steps. First, we will familiarze ourselves with the data set and understand what features we are working with. Next, we will explore our data set a little further with some visualizations to gain some first insights into what features might affect passenger survival. Then, we will prepare and tidy the data for machine learning. Finally we will select a machine learning algorithm and train our model. To do this, we will we will split our data set into two: a *training set* and *test set*. During training, the machine learning algorithm tries to find patterns in the training set that link features of each passenger to their survival. Once we have trained the model, we will use it to classify passengers from the test set as survived or not based on the their features. The test set will therefore allow us to assess the accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get familiar with the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start exploring, we need to import some libraries that will help us with our calculations, visualizations, and machine learning models. \n",
    "\n",
    "*Remember to press ***Shift+Enter*** to run each code cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An overview of the libraries used in this notebook is provided in the \"Sources\" section\n",
    "\n",
    "# Import data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# Import visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# Note: this cell and some of the cells below produce no visible output;\n",
    "# the sucessful execution of a cell is indicated by the number in the brackets [ ] on the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Check that only required modules are imported after ML section is finished </mark> \n",
    "\n",
    "Now, let's import our data set `titanic.csv` and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file into a new object called \"titanic_data\"\n",
    "titanic_data = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Show first 15 rows of data set\n",
    "titanic_data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each line in the list corresponds to one passenger. These data are a mixture of <b>categorical</b> and <b>numerical</b> features:\n",
    "\n",
    "`PassengerId`: Unique ID of the passenger\\\n",
    "`Survived`: Whether the passenger survived (1=Yes, 0=No)\\\n",
    "`Pclass`: Passenger's class (1=1st, 2=2nd, 3=3rd)\\\n",
    "`Name`: Passenger's name\\\n",
    "`Sex`: Passenger's sex (male or female)\\\n",
    "`Age`: Passenger's age in years\\\n",
    "`SibSp`: Number of siblings or spouses aboard the Titanic\\\n",
    "`Parch`: Number of parents or children aboard the Titanic\\\n",
    "`Ticket`: Ticket number\\\n",
    "`Fare`: Fare paid for ticket\\\n",
    "`Cabin`: Cabin number\\\n",
    "`Embarked`: Port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)\n",
    "\n",
    "From this we can already discern some information about passengers. For example, Mr. Owen Harris Braund was a 22-year-old man traveling in 3rd class who did not survive. Note that `NaN` is the abbreviation for \"Not a Number\". This is how Python represents missing data.\n",
    "\n",
    "*Hint: You can also take a look at the data set directly by double-clicking on `titanic.csv` in the Left Sidebar.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Q1 in the Answer.txt file</b>. \n",
    "    \n",
    "Why could missing data (\"NaN\") be problematic for machine learning models? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get an overview of some summary statistics about the numerical features in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics of the numerical features in the data set\n",
    "titanic_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summary includes the total `count` of values for each feature, their `mean`, their standard deviation `std`, the minimal and maximal values `min` and `max`, as well as the 25, 50 and 75 percentiles. The 50 percentile is the same as the median.\n",
    "\n",
    "We can already get some useful pieces of information from this table: Out of the 891 passengers in our data set, 38% survived (the survival rate is simply the `mean` of the `Survived` column because this column contains 1 if passengers survived, and 0 if they did not); more than half of the passengers were traveling in the 3rd class, and the majority were traveling alone. We can also see that we only have age data for 714 of 891 passengers in our data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the non-numerical, or categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics of the categorical features in the data set\n",
    "titanic_data.describe(include=[np.object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this table, `count` and `unique` are the total and unique number of values for a given feature, `top` is the most common value, and `freq` is the frequency of the most common value.\n",
    "\n",
    "Here, we can see that a majority of the passengers were male and embarked in Southampton. We also notice that `Cabin` data is missing for most passengers, which means we might not be able to use `Cabin` as a feature in our model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Further explore the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "In addition to descriptive statistics, **data visualization** can be a powerful tool. Visualizations can help identify potential issues in the data, and, importantly, improve our understanding of the problem, guiding experimentation. Let's now further explore our data using some visualizations.\n",
    "\n",
    "Remember that our aim is to predict passenger survival based on the information we have in our data set. For this, understanding the Titanic disaster and specifically what features might affect the outcome of survival is important. If you watched the movie Titanic, you would remember that women and children were given preference to lifeboats, and that the three passenger classes were not treated equally. This suggests that `Sex`, `Pclass`, and `Age` may be good predictors of survival. \n",
    "\n",
    "Let's first see how gender affects survival:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show bar plot of Sex vs survival rate \n",
    "sns.barplot(x='Sex',y='Survived',data=titanic_data,ci=None)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 70% of the female passengers survived, but only about 20% of the male passengers. Sex is therefore indeed a strong indicator of survival, and a trivial model deriving its predictions from just this one feature would likely already perform quite well! But we have a lot more data on each passenger than just gender and by considering multiple features, we should be able detect more complex patterns in the data that will, hopefully, allow us to improve the accuracy of our predictions. \n",
    "\n",
    "Let's look next at the relationship between `Pclass` and passenger survival:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show bar plot of Pclass vs survival rate \n",
    "sns.barplot(x='Pclass',y='Survived',data=titanic_data,ci=None)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trend is clear: Passengers in the 1st class had the highest, passengers in the 3rd class the lowest rate of survival. We can also look at both features `Sex` and `Pclass` simultaneously to verify these initial observations:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a nested barplot to show survival rate for both Sex and Pclass\n",
    "sns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=titanic_data, kind=\"bar\", ci=None)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  <b>Bonus Exercise</b>: Choose another feature and create a similar bar graph in the code cell below. Save your graph and describe any trends you observe: Does the selected feature predict survival? </div>\n",
    "\n",
    "<mark>Turn this into a regular exercise? </mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (create another bar graph for a factor of your choice)\n",
    "\n",
    "# Uncomment the next line to save your graph as a png\n",
    "# plt.savefig('feature_vs_survival.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now investigate how `Age` affects survival. As we have seen above, this feature is represented in a continuous numerical column, containing values from 0.42 to 80.0. We will therefore plot two histograms to compare visually the age distributions of those who survived with those who died: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two separate data sets for Survived and not Survived\n",
    "survived = titanic_data[titanic_data[\"Survived\"] == 1]\n",
    "died = titanic_data[titanic_data[\"Survived\"] == 0]\n",
    "\n",
    "# Draw histograms\n",
    "survived[\"Age\"].plot.hist(alpha=0.6,color='red',bins=50)\n",
    "died[\"Age\"].plot.hist(alpha=0.4,color='blue',bins=50)\n",
    "plt.legend(['Survived','Died'])\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the relationship is not obvious. The considerable fraction of missing `Age` values (only 714 of 891 values were given in our data) may further complicate the interpretation. From the given data, we can see that in some age ranges more passengers survived - where the red bars are higher than the blue bars - but drawing a clear conclusion is difficult. \n",
    "\n",
    "For a data set containing multiple features, visualizing several of them simultaneously quickly reaches its limits. One way to address this problem is to use a *correlation matrix* to show how each feature relates with the others. A correlation matrix contains values ranging from +1 (perfect correlation) to -1 (perfect anti-correlation) and is often displayed as a heat map, in which the strength of a relationship is shown as color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop PassengerId feature since no meaningful correlation is expected\n",
    "titanic_data_noId = titanic_data.drop(['PassengerId'], axis=1)\n",
    "\n",
    "# Compute correlation matrix\n",
    "corrMatrix = titanic_data_noId.corr()\n",
    "\n",
    "# Show heat map\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corrMatrix, annot=True, cmap=\"coolwarm\")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this heat map, darker colors represent a stronger positive (red: closer to 1) or negative (blue: closer to -1) correlation, and lighter colors represent a weaker correlation (closer to 0). The values on the diagonal are exactly 1, since the diagonal represents the correlation of features with themselves. \n",
    "\n",
    "Take a look at this heat map and check if you can confirm some of your expectations. We will come back to the correlation matrix after we have prepared and cleaned the data. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>QX in the Answers.txt file</b>.\n",
    "\n",
    "Which are the top 3 feature pairs that are most strongly (positively or negatively) correlated? Try to explain one of these relationships.\n",
    "    \n",
    "*Hint: Pay attention to the sign of the correlation!*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation is an important step in every data analysis pipeline, and often one of the most time-consuming tasks. Decision taken during data preparation benefit from a good understanding of the problem and play a critical role for the performance of the model. \n",
    "\n",
    "Raw data typically cannot be used from machine learning without preparation for several reasons. For example, most machine learning algorithms cannot work with missing data, prefer to work with numbers instead of text labels, and do not perform well when the input numerical attributes have very different scales. Therefore, preparing the data can entail different tasks: \n",
    "\n",
    "- Identifying and correcting mistakes or errors in the data\n",
    "- Dealing with missing data\n",
    "- Identifying features that are most relevant to the task\n",
    "- Removing features that are irrelevant to the task\n",
    "- Converting text labels into numbers\n",
    "- Changing the scale or distribution of features\n",
    "- Deriving new features from existing features (e.g., by combining features)\n",
    "\n",
    "However, not all data preparation tasks are always required for all data. Below, we will work through a few selected data preparation tasks that are relevant for our data set and the question we are trying to answer. Note that there are many ways to prepare and try to extract more information from this data set, as the [kaggle Titanic competition](https://www.kaggle.com/c/titanic) demonstrates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4A. Remove less relevant features\n",
    "\n",
    "We start by removing features which may not contribute much to our machine learning model or are problematic because they contain a lot of missing values or potential errors. Let's have again a look at all features and see which contain missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of missing values per feature \n",
    "column_names = titanic_data.columns\n",
    "for column in column_names:\n",
    "    print(column + ': ' + str(titanic_data[column].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see for the `Cabin` feature, the majority of values are missing (687 out of 891). We also miss 177 `Age` values and 2 `Embarked` values. We decide to fix the `Age` and `Embarked` columns later, but drop the `Cabin` feature. \n",
    "\n",
    "We also decide to remove `PassengerId`, `Name`, and `Ticket` since we do not expect that they would contribute much to our model, either because they do not encode meaningful information relevant to the probability of survival of a passenger, or that information is already reflected in some of the remaining features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features PassengerId, Name, Ticket, Cabin from data set\n",
    "titanic_data = titanic_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "# List remaining features\n",
    "list(titanic_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>QX in the Answers.txt file</b>.\n",
    "    \n",
    "For each of the features `PassengerId`, `Name`, and `Ticket`, can you think of a reason why they may or may not contribute to our model? \n",
    "If you were to delete a further feature from the data set, which one would you delete and why? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  <b>Bonus Question</b>: \n",
    "\n",
    "In general, what could be good reasons to confine ourselves to the most relevant features in the data?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4B. Convert text-based features \n",
    "\n",
    "Most machine learning algorithms cannot use text labels. Currently, two of our features text labels: `Sex` (\"male\" and \"female\") and `Embarked` (\"C\", \"Q\", \"S\" encoding the three ports Cherbourg, Queenstown, and Southampton). We replace these labels with numbers as follows:\n",
    "\n",
    "- `Sex`: male=0, female=1\n",
    "- `Embarked` column: S=0, C=1, Q=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Sex' labels with numbers\n",
    "titanic_data['Sex'].replace('male', 0 ,inplace=True)\n",
    "titanic_data['Sex'].replace('female', 1,inplace=True)\n",
    "\n",
    "# Replace 'Embarked' labels with numbers\n",
    "titanic_data['Embarked'].replace('S', 0,inplace=True)\n",
    "titanic_data['Embarked'].replace('C', 1,inplace=True)\n",
    "titanic_data['Embarked'].replace('Q', 2,inplace=True)\n",
    "\n",
    "# List feature types after conversion\n",
    "titanic_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are now numeric: integer `int64` or floating point numbers `float64`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  <b>Bonus Question</b>:    \n",
    "\n",
    "One issue with replacing the port designations S, C, and Q by 0, 1, and 2 is that machine learning algorithms will assume that *two nearby values are more similar than two distant values*. This may be fine in some cases (e.g. for ordered categories such as \"bad\", \"average\", \"good\", and \"excellent\"), but it is obviously not the case for the `Embarked` column. Can you come up with an alternative replacement scheme to avoid this problem? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4C. Fill in missing data\n",
    "\n",
    "We now address the missing data in the `Age` and `Embarked` columns. We have three options to deal with missing values: \n",
    "\n",
    "1. Remove all passenger rows which have missing values (NaN) from the data set \n",
    "2. Remove the whole feature\n",
    "3. Fill in the empty value with some value (zero, the mean, the median, etc.) \n",
    "\n",
    "To preserve as much data as possible, we choose option 3. For the `Age` column, we replace all missing values with the median age of the passengers; for the `Embarked`column, we replace the missing values with the mode (the most common value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplement missing `Age` data with median\n",
    "titanic_data['Age'].fillna(titanic_data['Age'].dropna().median(), inplace=True)\n",
    "\n",
    "# Supplement missing `Embarked` data with mode (most common value)\n",
    "freq_port = titanic_data.Embarked.dropna().mode()[0]\n",
    "titanic_data['Embarked'].fillna(freq_port, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>QX in the Answers.txt file</b>.\n",
    "    \n",
    "What could be better ways to replace the many missing age values than just using one single value (the median) for all of them?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4D. Derive new features \n",
    "\n",
    "Sometimes it can be helpful to derive new features from the original ones. This can mean to transform the scaling or the distribution of data of a given feature to make it more useful for the machine learning algorithm, or to combine two or more existing features to produce a more useful one--a process known as *feature engineering*.\n",
    "\n",
    "Here, we will only modify the `Age` and `Fare` columns by diving them into ranges, i.e. grouping their values into a few (numerical) categories. We also plot the histograms of the `Age` column to visualize the change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original `Age` distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(2,2,1)\n",
    "titanic_data['Age'].hist()\n",
    "plt.title(\"Original Age distribution\")\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of passengers')\n",
    ";\n",
    "\n",
    "# Split `Age` into five age groups: 0-16, 17-32, 33-48, 48-64, >64\n",
    "\n",
    "titanic_data['AgeBand'] = pd.cut(titanic_data['Age'], 5)\n",
    "titanic_data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n",
    "titanic_data.loc[ titanic_data['Age'] <= 16, 'Age'] = 0\n",
    "titanic_data.loc[(titanic_data['Age'] > 16) & (titanic_data['Age'] <= 32), 'Age'] = 1\n",
    "titanic_data.loc[(titanic_data['Age'] > 32) & (titanic_data['Age'] <= 48), 'Age'] = 2\n",
    "titanic_data.loc[(titanic_data['Age'] > 48) & (titanic_data['Age'] <= 64), 'Age'] = 3\n",
    "titanic_data.loc[ titanic_data['Age'] > 64, 'Age'] = 4\n",
    "titanic_data = titanic_data.drop(['AgeBand'], axis=1)\n",
    "\n",
    "# Show modified Age distribution\n",
    "plt.subplot(2,2,2)\n",
    "titanic_data['Age'].hist()\n",
    "plt.title(\"Modified Age distribution\")\n",
    "plt.xlabel('Age group')\n",
    "plt.ylabel('Number of passengers')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed similarly for the `Fare` column, using the 25%, 50%, and 75% percentiles to define the fare \"bands\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split `Fare` into four fare groups using the 25%, 50%, and 75% percentiles\n",
    "\n",
    "titanic_data['Fare'].fillna(titanic_data['Fare'].dropna().median(), inplace=True)\n",
    "titanic_data['FareBand'] = pd.qcut(titanic_data['Fare'], 4)\n",
    "titanic_data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n",
    "titanic_data.loc[ titanic_data['Fare'] <= 7.91, 'Fare'] = 0\n",
    "titanic_data.loc[(titanic_data['Fare'] > 7.91) & (titanic_data['Fare'] <= 14.45), 'Fare'] = 1\n",
    "titanic_data.loc[(titanic_data['Fare'] > 14.45) & (titanic_data['Fare'] <= 31), 'Fare'] = 2\n",
    "titanic_data.loc[ titanic_data['Fare'] > 31, 'Fare'] = 3\n",
    "titanic_data['Fare'] = titanic_data['Fare'].astype(int)\n",
    "titanic_data = titanic_data.drop(['FareBand'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Q4 in the Answers.txt file</b>.\n",
    "    \n",
    "Why do you think is it reasonable to split the `Age` and `Fare` features into groups?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned the data, let's save it as a separate file `titanic-clean.csv` and work directly with that file from now on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cleaned data set to new csv file\n",
    "titanic_data.to_csv('titanic-clean.csv', index=False)\n",
    "\n",
    "# Load cleaned data into new DataFrame object\n",
    "titanic_data_clean = pd.read_csv('titanic-clean.csv')\n",
    "\n",
    "# Show first 15 rows of cleaned data set\n",
    "titanic_data_clean.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take another brief look at the correlation matrix of the prepared data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corrMatrix = titanic_data_clean.corr()\n",
    "\n",
    "# Show heat map\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corrMatrix, annot=True, cmap=\"coolwarm\")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get now an overview of the features that we will use for modeling. We confirm the trends we noticed earlier: sex correlates with survival (women are more likely to have survived than man), and the passengers in first and second class were more likely to survive than those in third class. We can also see a negative correlation between `Fare` and `Pclass` (-0.63), and a positive correlation between `Fare` and `Survived` (0.3), which makes sense as first class tickets were the most expensive, and first class passengers were more likely to survive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now finally ready to train our machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark>WIP below</mark> \n",
    "\n",
    "## 5. Train and evaluate a model\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of a machine learning model is to make accurate predictions on new, previously unseen data. If we are building a model using the data set that contains what we want to predict, we need to divide the data set into two:\n",
    "\n",
    "- A <b>training</b> subset to train a model, which contains the information we are trying to predict \n",
    "- A <b>test</b> subset to test the model, which does not contain the information we are trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into training set and test set\n",
    "train_df, test_df = train_test_split(titanic_data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to separate survival, the outcome, from rest of the factors in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide each data set (training and test) into two parts: X & Y\n",
    "\n",
    "X_train = train_df.drop(\"Survived\", axis=1)\n",
    "Y_train = train_df[\"Survived\"]\n",
    "X_test = test_df.drop(\"Survived\", axis=1)\n",
    "Y_test = test_df[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a model. \n",
    "There are numerous predictive modeling algorithms but not all apply to our problem. Our problem is a classification and regression problem: we want to identify the relationship between passenger survival with other features (e.g., sex, age, class). We are also perfoming a category of machine learning called supervised learning as we are training our model with a given data set. Given this, we will take a closer look at 3(4?) algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a useful early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (in our case, Survival) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. \n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_pred = logreg.predict(X_test)\n",
    "acc_log = round(logreg.score(X_test, Y_test) * 100, 2)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Logistic Regression to confirm our assumptions by calculating the coefficient of the features in the function.\\\n",
    "Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame(train_df.columns.delete(0))\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n",
    "\n",
    "coeff_df.sort_values(by='Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sex is highest positive coefficient</b>, implying that as the Sex value increases (male = 0 to female = 1), the probability of Survived = 1 increases the most.\\\n",
    "<b>Pclass is the highest negative coefficient</b>, implying that as class increases (1-3), probability of Survived = 1 decreases the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree classifier maps features (tree branches) to conclusions about the target value (tree leaves, in our case, Survival). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. \n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "Y_pred = decision_tree.predict(X_test)\n",
    "acc_decision_tree = round(decision_tree.score(X_test, Y_test) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "random_forest.score(X_train, Y_train)\n",
    "acc_random_forest = round(random_forest.score(X_test, Y_test) * 100, 2)\n",
    "acc_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbors classifier (K-NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-NN classifier is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. \n",
    "\n",
    "Source:https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "acc_knn = round(knn.score(X_test, Y_test) * 100, 2)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see how well the chosen model predicts our data.\\\n",
    "The function scoretakes the values of the test data set (X_test), calculates with the model the corresponding values for the survival status, and compares them with the correct values (Y_test). The output value `acc_logof` is the probability that the model predicted survival status correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model and calculate accuracy\n",
    "\n",
    "acc_random_forest = round(random_forest.score(X_test, Y_test) * 100, 2)\n",
    "\n",
    "print(\"The accuracy of the model with respect to the test data is:\")\n",
    "print(acc_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Q6 on the U4I platform (Bonus Question)</b>.\n",
    "    \n",
    "Run any of machine learning algorithm several rimes in a row (without changing the code). Why do you get a different accuracy each time? More here?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have completed the Titanic Challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "- https://www.kaggle.com/c/titanic\n",
    "- https://www.kaggle.com/startupsci/titanic-data-science-solutions\n",
    "\n",
    "Sources for pictures:\n",
    "- Titanic.jpg: https://upload.wikimedia.org/wikipedia/commons/9/92/Titanic.jpg\n",
    "\n",
    "### Python packages used\n",
    "\n",
    "This notebook uses several standard Python packages. These are:\n",
    "\n",
    "* **pandas** is a powerful data analysis package, providing the \"DataFrame\" structure to store data in memory and work with it easily and efficiently. DataFrame is a 2-dimensional labeled data structure with columns of potentially different types; you can think of it like a spreadsheet.\n",
    "* **numpy** is an essential package for scientific computing with Python, providing a fast numerical array structure and helper functions.\n",
    "* **random** generates pseudo-random numbers for various distributions.\n",
    "* **matplotlib** is the basic plotting library in Python; most other Python plotting libraries are built on top of it.\n",
    "* **seaborn** is an advanced statistical plotting library.\n",
    "* **sklearn** (**scikit-learn**) is an essential Machine Learning package in Python.\n",
    "\n",
    "<mark>To be included</mark>\n",
    "\n",
    "- ipywidgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark>===Internal notes section, to be removed in final Challenge===</mark>\n",
    "\n",
    "### <mark> Check ALL code! </mark>\n",
    "\n",
    "### <mark> User interactivity </mark>\n",
    "\n",
    "- Ask user to write some/any code, e.g. \"data = pd.read_csv('<file name here>')\"? Trivial, but might add to the feeling that you're doing sth. yourself\n",
    "- (\"GUI\": Answering questions: Text fields?)\n",
    "\n",
    "\n",
    "### <mark> Approaches to improve the accuracy of the model </mark>\n",
    "\n",
    "Learn more about the data > improve understanding > guide experimentation:\n",
    "\n",
    "- Feature engineering: Design / create some new features\n",
    "- try different types of preprocessing (different methods to fill in missing values)\n",
    "- try different types of ML models, then combine / ensemble them\n",
    "- Learn from other's code\n",
    "\n",
    "\n",
    "\n",
    "## <mark> To Dos </mark>\n",
    "    \n",
    "- Prepare Answer.txt\n",
    "- ...\n",
    "- \n",
    "    \n",
    "    \n",
    "## <mark> Idea collection for Advanced Version </mark>\n",
    "    \n",
    "- Use Kaggles Test data set to generate realistic Kaggle scores\n",
    "- Use cross-correlation\n",
    "- Use one-hot encoding\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
