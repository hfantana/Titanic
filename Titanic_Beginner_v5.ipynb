{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Titanic Challenge (Beginner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: You do not need to understand the Python code or be able to write code to complete this tutorial and pass the Challenge.\n",
    "#### Remember to hit Shift+Enter in all the code cells to execute the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">A cell like this indicates a question you need to answer for this Challenge on the U4I platform. Please answer the question <b>before</b> continuing through the notebook.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "[1. Introduction](#1.-Introduction)\n",
    "\n",
    "[2. Get familiar with the data](#2.-Get-familiar-with-the-data)\n",
    "\n",
    "[3. Prepare the data](#3.-Prepare-the-data)\n",
    "   - [3a. Remove some features](#3a.-Remove-some-features)\n",
    "   - [3b. Replace strings](#3b.-Replace-strings)\n",
    "   - [3c. Fill in missing data](#3c.-Fill-in-missing-data)\n",
    "   - [3d. Combine features](#3d.-Combine-features)\n",
    "   \n",
    "[4. Visualize the data](#4.-Visualize-the-data)\n",
    "\n",
    "[5. Create a machine learning model](#5.-Create-a-machine-learning-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered \"unsinkable\" RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren't enough lifeboats for everyone on board, resulting in the death of 1502 of the 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some were more likely to survive than others.\n",
    "\n",
    "In this Challenge, you will build a predictive model to answer the question \"<b>Who was more likely to survive on the Titanic?</b>\" using passenger data. \n",
    "\n",
    "To do this, we will use a subset of the passenger data (891 of the 2224 passengers and crew on board), which includes passenger survival. Once we have trained our predictive model, we will test it on a separate subset of the passenger data (418 of the passengers and crew on board), which does not include passenger survival, to determine the prediction accuracy of the model we developed. \n",
    "\n",
    "Source: https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get familiar with the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start exploring, we need to import some libraries that will help us with our calculations, visualizations, and machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# Import visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the data set `Titanic_1` and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set, call it \"data\"\n",
    "data = pd.read_csv('Titanic_1.csv')\n",
    "\n",
    "# Show first 15 rows of data set\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each line in the list corresponds to one passenger. These data are a mixture of <b>categorical</b> and <b>numerical</b> features:\n",
    "\n",
    "`PassengerId`: Unique ID of the passenger\\\n",
    "`Survived`: Survived (1) or died (0)\\\n",
    "`Pclass`: Passenger's class (1st, 2nd, or 3rd)\\\n",
    "`Name`: Passenger's name\\\n",
    "`Sex`: Passenger's sex (male or female)\\\n",
    "`Age`: Passenger's age\\\n",
    "`SibSp`: Number of siblings / spouses aboard the Titanic\\\n",
    "`Parch`: Number of parents / children aboard the Titanic\\\n",
    "`Ticket`: Ticket number\\\n",
    "`Fare`: Fare paid for ticket\\\n",
    "`Cabin`: Cabin number\\\n",
    "`Embarked`: Where the passenger got on the ship (C - Cherbourg, S - Southampton, Q - Queenstown)\n",
    "\n",
    "From this we can already discern some information about passengers. For example, Braund Owen Harris was a 22-year-old man traveling in 3rd class who did not survive.  \n",
    "\n",
    "Note: \"NaN\" is the abbreviation for \"Not a Number\". This is how Python represents missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Q1 on the U4I platform</b>. \n",
    "    \n",
    "Question 1: Why could missing data (NaNs) be problematic for machine learning models? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get an overview of the numerical features of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we need this?\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we need this?\n",
    "data.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we work with any machine learning models, we need to ensure the data is prepared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Remove some features\n",
    "\n",
    "The first step is to <b>remove some features</b> by answering the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which features contain blank, null, or empty values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show missing values in data set\n",
    "column_names = data.columns\n",
    "for column in column_names:\n",
    "    print(column + ': ' + str(data[column].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `Cabin` has the most missing values (687), followed by `Age` (177), and then `Embarked` (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which features are mixed data types?\n",
    "`Ticket` is a mix of numeric and alphanumeric data types. `Cabin` is alphanumeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which features may contain errors or typos?\n",
    "`Name` might contain errors as are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we will remove the features `Ticket`, `Cabin`, and `Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features Ticket, Cabin, Name from data set\n",
    "data = data.drop(['Ticket', 'Cabin', 'Name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Q2 on the U4I platform</b>.\n",
    "    \n",
    "    Why did we keep PassengerId in the data set? Remember: We want to train a model to predict survival and test our model on a data set that does not include survival.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Q3 on the U4I platform</b>.\n",
    "    \n",
    "    If you could delete another varaible or column from the data set, which one would you delete and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Replace strings \n",
    "\n",
    "Next, we need to <b>replace strings (text or letter sequences) with numbers</b> \n",
    "\n",
    "This is because the machine learning algorithms we will use cannot process words. We will replace female with 1, male with 0, S with 0, C with 1, and Q with 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace strings with numbers\n",
    "\n",
    "data['Sex'].replace('female', 1,inplace=True)\n",
    "data['Sex'].replace('male', 0 ,inplace=True)\n",
    "data['Embarked'].replace('S', 0,inplace=True)\n",
    "data['Embarked'].replace('C', 1,inplace=True)\n",
    "data['Embarked'].replace('Q', 2,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. Fill in missing data\n",
    "\n",
    "Finally, we need to deal with in </b>missing data</b>.\n",
    "\n",
    "Data records are not always complete and this is also true for our data set. Missing data can interfere with machine learning algorithms so we need to <b>fill in the missing data</b>. One way to fill in missing values by using the available values in the data set (e.g., mean or average value, median or middle value, mode or most common value), and approximating a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplement missing data in Age with median and Embarked with mode (most common value)\n",
    "\n",
    "data['Age'].fillna(data['Age'].dropna().median(), inplace=True)\n",
    "freq_port = data.Embarked.dropna().mode()[0]\n",
    "data['Embarked'].fillna(freq_port, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3d. Combine features \n",
    "\n",
    "Sometimes it can be useful to <b>combine features into new features</b> for visualizations and calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data categories for Age and Fare\n",
    "\n",
    "# Create 5 age groups\n",
    "\n",
    "data['AgeBand'] = pd.cut(data['Age'], 5)\n",
    "data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n",
    "data.loc[ data['Age'] <= 16, 'Age'] = 0\n",
    "data.loc[(data['Age'] > 16) & (data['Age'] <= 32), 'Age'] = 1\n",
    "data.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age'] = 2\n",
    "data.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age'] = 3\n",
    "data.loc[ data['Age'] > 64, 'Age'] = 4\n",
    "data = data.drop(['AgeBand'], axis=1)\n",
    "\n",
    "# Create 4 fare groups\n",
    "\n",
    "data['Fare'].fillna(data['Fare'].dropna().median(), inplace=True)\n",
    "data['FareBand'] = pd.qcut(data['Fare'], 4)\n",
    "data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n",
    "data.loc[ data['Fare'] <= 7.91, 'Fare'] = 0\n",
    "data.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454), 'Fare'] = 1\n",
    "data.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31), 'Fare'] = 2\n",
    "data.loc[ data['Fare'] > 31, 'Fare'] = 3\n",
    "data['Fare'] = data['Fare'].astype(int)\n",
    "data = data.drop(['FareBand'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Q4 on the U4I platform</b>.\n",
    "    \n",
    "    Why did we group Age and Fare into groups? Answer: More data per class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Q5 on the U4I platform (Bonus Question)</b>.\n",
    "    \n",
    "    The port designations S, C and Q were replaced by 0, 1 and 2. Linear models, such as perceptrons, assume that at a higher variable value the survival probability either increases or decreases. What is the problem with our approach (replacing port designations with 0, 1, and 2)? How can we avoid this problem? Think about how to do it better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 15 rows of new data set\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing data is a great way to gain some insights and see some trends before applying any machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to predict survival, it makes sense to visualize the relationship between some of the factors and survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class vs. Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Pclass').Survived.mean().plot(kind='bar')\n",
    "sns.barplot(x='Pclass', y='Survived', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex vs. Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Sex').Survived.mean().plot(kind='bar')\n",
    "sns.barplot(x='Sex', y='Survived', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class and Sex  vs. Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.crosstab(data['Pclass'], data['Sex'])\n",
    "tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\n",
    "plt.xlabel('Pclass')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarked vs. Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Embarked').Survived.mean().plot(kind='bar')\n",
    "sns.barplot(x='Embarked', y='Survived', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarked, Class, and Sex vs. Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot(x='Pclass', y='Survived', hue='Sex', col='Embarked', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age vs. Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Age').Survived.mean().plot(kind='bar')\n",
    "sns.barplot(x='Age', y='Survived', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age and Embarked vs. Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "sns.violinplot(x=\"Embarked\", y=\"Age\", hue=\"Survived\", data=data, split=True, ax=ax1)\n",
    "sns.violinplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=data, split=True, ax=ax2)\n",
    "sns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=data, split=True, ax=ax3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>#HF how many more visualizations do we need/should we have. Either way I think at the end of this section we should have a Q that asks the users to use the visualizations to make some conclusions as to what affects survival. This can be a basis for how to improve the model in the last question. </mark> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a machine learning model\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of a machine learning model is to make accurate predictions on new, previously unseen data. If we are building a model using the data set that contains what we want to predict, we need to divide the data set into two:\n",
    "\n",
    "- A <b>training</b> subset to train a model, which contains the information we are trying to predict \n",
    "- A <b>test</b> subset to test the model, which does not contain the information we are trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into training set and test set\n",
    "train_df, test_df = train_test_split(data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to separate survival, the outcome, from rest of the factors in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide each data set (training and test) into two parts: X & Y\n",
    "\n",
    "X_train = train_df.drop(\"Survived\", axis=1)\n",
    "Y_train = train_df[\"Survived\"]\n",
    "X_test = test_df.drop(\"Survived\", axis=1)\n",
    "Y_test = test_df[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a model. \n",
    "There are numerous predictive modeling algorithms but not all apply to our problem. Our problem is a classification and regression problem: we want to identify the relationship between passenger survival with other features (e.g., sex, age, class). We are also perfoming a category of machine learning called supervised learning as we are training our model with a given data set. Given this, we will take a closer look at 3(4?) algorithms: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a useful early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (in our case, Survival) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. \n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_pred = logreg.predict(X_test)\n",
    "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Logistic Regression to confirm our assumptions by calculating the coefficient of the features in the function.\\\n",
    "Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame(train_df.columns.delete(0))\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n",
    "\n",
    "coeff_df.sort_values(by='Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sex is highest positive coefficient</b>, implying that as the Sex value increases (male = 0 to female = 1), the probability of Survived = 1 increases the most.\\\n",
    "<b>Pclass is the highest negative coefficient</b>, implying that as class increases (1-3), probability of Survived = 1 decreases the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree classifier maps features (tree branches) to conclusions about the target value (tree leaves, in our case, Survival). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. \n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "Y_pred = decision_tree.predict(X_test)\n",
    "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "random_forest.score(X_train, Y_train)\n",
    "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
    "acc_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbors classifier (K-NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-NN classifier is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. \n",
    "\n",
    "Source:https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see how well the chosen model predicts our data.\\\n",
    "The function scoretakes the values of the test data set (X_test), calculates with the model the corresponding values for the survival status, and compares them with the correct values (Y_test). The output value `acc_logof` is the probability that the model predicted survival status correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model and calculate accuracy\n",
    "\n",
    "acc_random_forest = round(model.score(X_test, Y_test) * 100, 2)\n",
    "\n",
    "print(\"\\n  \\nThe accuracy of the model with respect to the test data is:\")\n",
    "print(acc_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Q6 on the U4I platform (Bonus Question)</b>.\n",
    "    \n",
    "    Run any of machine learning algorithm several rimes in a row (without changing the code). Why do you get a different accuracy each time? More here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have completed the Titanic Challenge (Beginner)! Remember to submit the exercise on the U4I platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources:\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "https://www.kaggle.com/startupsci/titanic-data-science-solutions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
