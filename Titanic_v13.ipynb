{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster\n",
    "\n",
    "##### Based on the famous [kaggle Titanic competition](https://www.kaggle.com/c/titanic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal: Work through a simple machine learning example from start to finish along a typical data analysis pipeline. \n",
    "\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered \"unsinkable\" RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren't enough lifeboats for everyone on board, resulting in the death of 1514 of the 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some were more likely to survive than others. We will use machine learning to create a model that predicts which passengers survived the Titanic shipwreck. \n",
    "\n",
    "<div align=\"center\">\n",
    "<img width=\"1200\" title=\"Titanic at Southampton docks, prior to departure\" src=\"images/titanic.jpg\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note**: This exercise is based on a Jupyter notebook, an interactive environment for writing and running code, and is running in Python. To get familiar with working in Jupyter notebooks, see our short \"JupyterLab Tutorial\". For a basic introduction to programming in Python, see the \"Introduction to Python\" notebook.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">A cell like this indicates a question you need to answer in the Answer.txt file. Please answer the question <b>before</b> continuing through the notebook. You can <b>double click on Answer.txt</b> in the Left Sidebar now to open it in a new tab. As you go through the notebook, navigate between the tabs to answer questions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#1.-Introduction)\n",
    "\n",
    "2. [Get familiar with the data](#2.-Get-familiar-with-the-data)\n",
    "\n",
    "3. [Further explore the data](#3.-Further-explore-the-data)\n",
    "\n",
    "4. [Prepare the data](#4.-Prepare-the-data)\n",
    "\n",
    "   1. [Remove less relevant features](#4A.-Remove-less-relevant-features)\n",
    "   2. [Convert text-based features](#4B.-Convert-text-based-features)\n",
    "   3. [Fill in missing data](#4C.-Fill-in-missing-data)\n",
    "   4. [Derive new features](#4D.-Derive-new-features)\n",
    "   \n",
    "   \n",
    "5. [Train and evaluate a model](#5.-Train-and-evaluate-a-model)\n",
    "\n",
    "6. [Sources](#Sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Challenge, we will build a predictive model to answer the question **\"Who was more likely to survive on the Titanic?\"** using passenger data. These data contain information about each passenger (e.g., the passenger's name, age, gender, ticket class, etc.). For this exercise, we will use a subset of the full passenger data set (891 of the 2224 passengers and crew on board). Importantly, our data set `titanic.csv` contains also the information whether each passenger survived or not, i.e. the *label* required for using supervised machine learning methods.\n",
    "\n",
    "To build our model we will work through a number of steps. First, we will familiarze ourselves with the data set and understand what features we are working with. Next, we will explore our data set a little further with some visualizations to gain some first insights into what features might affect passenger survival. Then, we will prepare and tidy the data for machine learning. Finally we will select a machine learning algorithm, and train and assess our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get familiar with the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start exploring, we need to import some libraries that will help us with our calculations and visualizations. \n",
    "\n",
    "*Remember to press ***Shift+Enter*** to run each code cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An overview of the libraries used in this notebook is provided in the \"Sources\" section\n",
    "\n",
    "# Import data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# Import visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Note: this cell and some of the cells below produce no visible output;\n",
    "# the sucessful execution of a code cell is indicated by the number in the brackets [ ] on the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import our data set `titanic.csv` and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file into a new object called \"titanic_data\"\n",
    "titanic_data = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Show first 15 rows of data set\n",
    "titanic_data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each line in the list corresponds to one passenger. These data are a mixture of <b>categorical</b> and <b>numerical</b> features:\n",
    "\n",
    "`PassengerId`: Unique ID of the passenger\\\n",
    "`Survived`: Whether the passenger survived (1=Yes, 0=No)\\\n",
    "`Pclass`: Passenger's class (1=1st, 2=2nd, 3=3rd)\\\n",
    "`Name`: Passenger's name\\\n",
    "`Sex`: Passenger's sex (male or female)\\\n",
    "`Age`: Passenger's age in years\\\n",
    "`SibSp`: Number of siblings or spouses aboard the Titanic\\\n",
    "`Parch`: Number of parents or children aboard the Titanic\\\n",
    "`Ticket`: Ticket number\\\n",
    "`Fare`: Fare paid for ticket\\\n",
    "`Cabin`: Cabin number\\\n",
    "`Embarked`: Port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)\n",
    "\n",
    "From this we can already discern some information about passengers. For example, Mr. Owen Harris Braund was a 22-year-old man traveling in 3rd class who did not survive. Note that `NaN` is the abbreviation for \"Not a Number\". This is how Python represents missing data.\n",
    "\n",
    "*Hint: You can also take a look at the data set directly by double-clicking on `titanic.csv` in the Left Sidebar.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Question 1</b> in the Answer.txt file. \n",
    "    \n",
    "Why could missing data (\"NaN\") be problematic for machine learning models? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get an overview of some summary statistics about the numerical features in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics of the numerical features in the data set\n",
    "titanic_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summary includes the total `count` of values for each feature, their `mean`, their standard deviation `std`, the minimal and maximal values `min` and `max`, as well as the 25, 50 and 75 percentiles. The 50 percentile is the same as the median.\n",
    "\n",
    "We can already get some useful pieces of information from this table: Out of the 891 passengers in our data set, 38% survived (the survival rate is simply the `mean` of the `Survived` column because this column contains 1 if passengers survived, and 0 if they did not); more than half of the passengers were traveling in the 3rd class, and the majority were traveling alone. We can also see that we only have age data for 714 of 891 passengers in our data set. \n",
    "\n",
    "Let's also look at the non-numerical, or categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics of the categorical features in the data set\n",
    "titanic_data.describe(include=[np.object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this table, `count` and `unique` are the total and unique number of values for a given feature, `top` is the most common value, and `freq` is the frequency of the most common value.\n",
    "\n",
    "Here, we can see that a majority of the passengers were male and embarked in Southampton. We also notice that `Cabin` data is missing for most passengers, which means we might not be able to use `Cabin` as a feature in our model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Further explore the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "In addition to descriptive statistics, **data visualization** can be a powerful tool. Visualizations can help identify potential issues in the data, and, importantly, improve our understanding of the problem, guiding experimentation. Let's now further explore our data using some visualizations.\n",
    "\n",
    "Remember that our aim is to predict passenger survival based on the information we have in our data set. For this, understanding the Titanic disaster and specifically what features might affect the outcome of survival is important. If you watched the movie Titanic, you would remember that women and children were given preference to lifeboats, and that the three passenger classes were not treated equally. This suggests that `Sex`, `Pclass`, and `Age` may be good predictors of survival. \n",
    "\n",
    "Let's first see how gender affects survival:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show bar plot of Sex vs survival rate \n",
    "sns.barplot(x='Sex',y='Survived',data=titanic_data,ci=None)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 70% of the female passengers survived, but only about 20% of the male passengers. Sex is therefore indeed a strong indicator of survival, and a trivial model deriving its predictions from just this one feature would likely already perform quite well! But we have a lot more data on each passenger than just gender and by considering multiple features, we should be able detect more complex patterns in the data that will, hopefully, allow us to improve the accuracy of our predictions. \n",
    "\n",
    "Let's look next at the relationship between `Pclass` and passenger survival:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show bar plot of Pclass vs survival rate \n",
    "sns.barplot(x='Pclass',y='Survived',data=titanic_data,ci=None)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trend is clear: Passengers in the 1st class had the highest, passengers in the 3rd class the lowest rate of survival. We can also look at both features `Sex` and `Pclass` simultaneously to verify these initial observations:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a nested barplot to show survival rate for both Sex and Pclass\n",
    "sns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=titanic_data, kind=\"bar\", ci=None)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  <b>Bonus Question 1</b>: \n",
    "    \n",
    "Choose another feature and create a similar bar graph in the code cell below. Save your graph and describe any trends you observe: Does the selected feature predict survival? </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (create another bar graph for a factor of your choice)\n",
    "\n",
    "# Uncomment the next line to save your graph as a png\n",
    "# plt.savefig('feature_vs_survival.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now investigate how `Age` affects survival. As we have seen above, this feature is represented in a continuous numerical column, containing values from 0.42 to 80.0. We will therefore plot two histograms to compare visually the age distributions of those who survived with those who died: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two separate data sets for Survived and not Survived\n",
    "survived = titanic_data[titanic_data[\"Survived\"] == 1]\n",
    "died = titanic_data[titanic_data[\"Survived\"] == 0]\n",
    "\n",
    "# Draw histograms\n",
    "survived[\"Age\"].plot.hist(alpha=0.6,color='red',bins=50)\n",
    "died[\"Age\"].plot.hist(alpha=0.4,color='blue',bins=50)\n",
    "plt.legend(['Survived','Died'])\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the relationship is not obvious. The considerable fraction of missing `Age` values (only 714 of 891 values were given in our data) may further complicate the interpretation. From the given data, we can see that in some age ranges more passengers survived - where the red bars are higher than the blue bars - but drawing a clear conclusion is difficult. \n",
    "\n",
    "For a data set containing multiple features, visualizing several of them simultaneously quickly reaches its limits. One way to address this problem is to use a *correlation matrix* to show how each feature relates with the others. A correlation matrix contains values ranging from +1 (perfect correlation) to -1 (perfect anti-correlation) and is often displayed as a heat map, in which the strength of a relationship is shown as color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop PassengerId feature since no meaningful correlation is expected\n",
    "titanic_data_noId = titanic_data.drop(['PassengerId'], axis=1)\n",
    "\n",
    "# Compute correlation matrix\n",
    "corrMatrix = titanic_data_noId.corr()\n",
    "\n",
    "# Show heat map\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corrMatrix, annot=True, cmap=\"coolwarm\")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this heat map, darker colors represent a stronger positive (red: closer to 1) or negative (blue: closer to -1) correlation, and lighter colors represent a weaker correlation (closer to 0). The values on the diagonal are exactly 1, since the diagonal represents the correlation of features with themselves. \n",
    "\n",
    "Take a look at this heat map and check if you can confirm some of your expectations. We will come back to the correlation matrix after we have prepared and cleaned the data. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Question 2</b> in the Answer.txt file.\n",
    "\n",
    "Which are the top 3 feature pairs that are most strongly (positively or negatively) correlated? Try to explain one of these relationships.\n",
    "    \n",
    "*Hint: Pay attention to the sign of the correlation!*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation is an important step in every data analysis pipeline, and often one of the most time-consuming tasks. Decision taken during data preparation benefit from a good understanding of the problem and play a critical role for the performance of the model. \n",
    "\n",
    "Raw data typically cannot be used from machine learning without preparation for several reasons. For example, most machine learning algorithms cannot work with missing data, prefer to work with numbers instead of text labels, and do not perform well when the input numerical attributes have very different scales. Therefore, preparing the data can entail different tasks: \n",
    "\n",
    "- Identifying and correcting mistakes or errors in the data\n",
    "- Dealing with missing data\n",
    "- Identifying features that are most relevant to the task\n",
    "- Removing features that are irrelevant to the task\n",
    "- Converting text labels into numbers\n",
    "- Changing the scale or distribution of features\n",
    "- Deriving new features from existing features (e.g., by combining features)\n",
    "\n",
    "However, not all data preparation tasks are always required for all data. Below, we will work through a few selected data preparation tasks that are relevant for our data set and the question we are trying to answer. Note that there are many ways to prepare and try to extract more information from this data set, as the [kaggle Titanic competition](https://www.kaggle.com/c/titanic) demonstrates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4A. Remove less relevant features\n",
    "\n",
    "We start by removing features which may not contribute much to our machine learning model or are problematic because they contain a lot of missing values or potential errors. Let's have again a look at all features and see which contain missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of missing values per feature \n",
    "column_names = titanic_data.columns\n",
    "for column in column_names:\n",
    "    print(column + ': ' + str(titanic_data[column].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see for the `Cabin` feature, the majority of values are missing (687 out of 891). We also miss 177 `Age` values and 2 `Embarked` values. We decide to fix the `Age` and `Embarked` columns later, but drop the `Cabin` feature. \n",
    "\n",
    "We also decide to remove `PassengerId`, `Name`, and `Ticket` since we do not expect that they would contribute much to our model, either because they do not encode meaningful information relevant to the probability of survival of a passenger, or that information is already reflected in some of the remaining features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features PassengerId, Name, Ticket, Cabin from data set\n",
    "titanic_data = titanic_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "# List remaining features\n",
    "list(titanic_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Question 3</b> in the Answer.txt file.\n",
    "\n",
    "For each of the features `PassengerId`, `Name`, and `Ticket`, can you think of a reason why they may or may not contribute to our model? \n",
    "If you were to delete a further feature from the data set, which one would you delete and why? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  <b>Bonus Question 2</b>: \n",
    "\n",
    "In general, what could be good reasons to confine ourselves to the most relevant features in the data?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4B. Convert text-based features \n",
    "\n",
    "Most machine learning algorithms cannot use text labels. Currently, two of our features text labels: `Sex` (\"male\" and \"female\") and `Embarked` (\"C\", \"Q\", \"S\" encoding the three ports Cherbourg, Queenstown, and Southampton). We replace these labels with numbers as follows:\n",
    "\n",
    "- `Sex`: male=0, female=1\n",
    "- `Embarked` column: S=0, C=1, Q=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Sex' labels with numbers\n",
    "titanic_data['Sex'].replace('male', 0 ,inplace=True)\n",
    "titanic_data['Sex'].replace('female', 1,inplace=True)\n",
    "\n",
    "# Replace 'Embarked' labels with numbers\n",
    "titanic_data['Embarked'].replace('S', 0,inplace=True)\n",
    "titanic_data['Embarked'].replace('C', 1,inplace=True)\n",
    "titanic_data['Embarked'].replace('Q', 2,inplace=True)\n",
    "\n",
    "# List feature types after conversion\n",
    "titanic_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are now numeric: integer `int64` or floating point numbers `float64`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  <b>Bonus Question 3</b>:    \n",
    "\n",
    "One issue with replacing the port designations S, C, and Q by 0, 1, and 2 is that machine learning algorithms will assume that *two nearby values are more similar than two distant values*. This may be fine in some cases (e.g. for ordered categories such as \"bad\", \"average\", \"good\", and \"excellent\"), but it is obviously not the case for the `Embarked` column. Can you come up with an alternative replacement scheme to avoid this problem? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4C. Fill in missing data\n",
    "\n",
    "We now address the missing data in the `Age` and `Embarked` columns. We have three options to deal with missing values: \n",
    "\n",
    "1. Remove all passenger rows which have missing values (NaN) from the data set \n",
    "2. Remove the whole feature\n",
    "3. Fill in the empty value with some value (zero, the mean, the median, etc.) \n",
    "\n",
    "To preserve as much data as possible, we choose option 3. For the `Age` column, we replace all missing values with the median age of the passengers; for the `Embarked`column, we replace the missing values with the mode (the most common value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplement missing `Age` data with median\n",
    "titanic_data['Age'].fillna(titanic_data['Age'].dropna().median(), inplace=True)\n",
    "\n",
    "# Supplement missing `Embarked` data with mode (most common value)\n",
    "freq_port = titanic_data.Embarked.dropna().mode()[0]\n",
    "titanic_data['Embarked'].fillna(freq_port, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Question 4</b> in the Answer.txt file.\n",
    "    \n",
    "What could be better ways to replace the many missing age values than just using one single value (the median) for all of them?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4D. Derive new features \n",
    "\n",
    "Sometimes it can be helpful to derive new features from the original ones. This can mean to transform the scaling or the distribution of data of a given feature to make it more useful for the machine learning algorithm, or to combine two or more existing features to produce a more useful one--a process known as *feature engineering*.\n",
    "\n",
    "Here, we will only modify the `Age` and `Fare` columns by diving them into ranges, i.e. grouping their values into a few (numerical) categories. We also plot the histograms of the `Age` column to visualize the change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original `Age` distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(2,2,1)\n",
    "titanic_data['Age'].hist()\n",
    "plt.title(\"Original Age distribution\")\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of passengers')\n",
    ";\n",
    "\n",
    "# Split `Age` into five age groups: 0-16, 17-32, 33-48, 48-64, >64\n",
    "\n",
    "titanic_data['AgeBand'] = pd.cut(titanic_data['Age'], 5)\n",
    "titanic_data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n",
    "titanic_data.loc[ titanic_data['Age'] <= 16, 'Age'] = 0\n",
    "titanic_data.loc[(titanic_data['Age'] > 16) & (titanic_data['Age'] <= 32), 'Age'] = 1\n",
    "titanic_data.loc[(titanic_data['Age'] > 32) & (titanic_data['Age'] <= 48), 'Age'] = 2\n",
    "titanic_data.loc[(titanic_data['Age'] > 48) & (titanic_data['Age'] <= 64), 'Age'] = 3\n",
    "titanic_data.loc[ titanic_data['Age'] > 64, 'Age'] = 4\n",
    "titanic_data = titanic_data.drop(['AgeBand'], axis=1)\n",
    "\n",
    "# Show modified `Age` distribution\n",
    "plt.subplot(2,2,2)\n",
    "titanic_data['Age'].hist()\n",
    "plt.title(\"Modified Age distribution\")\n",
    "plt.xlabel('Age group')\n",
    "plt.ylabel('Number of passengers')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed similarly for the `Fare` column, using the 25%, 50%, and 75% percentiles to define the fare \"bands\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split `Fare` into four fare groups using the 25%, 50%, and 75% percentiles\n",
    "\n",
    "titanic_data['Fare'].fillna(titanic_data['Fare'].dropna().median(), inplace=True)\n",
    "titanic_data['FareBand'] = pd.qcut(titanic_data['Fare'], 4)\n",
    "titanic_data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n",
    "titanic_data.loc[ titanic_data['Fare'] <= 7.91, 'Fare'] = 0\n",
    "titanic_data.loc[(titanic_data['Fare'] > 7.91) & (titanic_data['Fare'] <= 14.45), 'Fare'] = 1\n",
    "titanic_data.loc[(titanic_data['Fare'] > 14.45) & (titanic_data['Fare'] <= 31), 'Fare'] = 2\n",
    "titanic_data.loc[ titanic_data['Fare'] > 31, 'Fare'] = 3\n",
    "titanic_data['Fare'] = titanic_data['Fare'].astype(int)\n",
    "titanic_data = titanic_data.drop(['FareBand'], axis=1)\n",
    "\n",
    "# Check unique values of modified `Fare` \n",
    "np.unique(titanic_data['Fare'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Pause! Answer <b>Question 5</b> in the Answer.txt file.\n",
    "    \n",
    "Why do you think is it reasonable to split the `Age` and `Fare` features into groups?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned the data, let's save it as a separate file `titanic-clean.csv` and work directly with that file from now on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cleaned data set to new csv file\n",
    "titanic_data.to_csv('titanic-clean.csv', index=False)\n",
    "\n",
    "# Load cleaned data into new DataFrame object\n",
    "titanic_data_clean = pd.read_csv('titanic-clean.csv')\n",
    "\n",
    "# Show first 15 rows of cleaned data set\n",
    "titanic_data_clean.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take another brief look at the correlation matrix of the prepared data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corrMatrix = titanic_data_clean.corr()\n",
    "\n",
    "# Show heat map\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corrMatrix, annot=True, cmap=\"coolwarm\")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get now an overview of the features that we will use for modeling. We confirm the trends we noticed earlier: sex correlates with survival (women are more likely to have survived than man), and the passengers in first and second class were more likely to survive than those in third class. We can also see a negative correlation between `Fare` and `Pclass` (-0.63), and a positive correlation between `Fare` and `Survived` (0.3), which makes sense as first class tickets were the most expensive, and first class passengers were more likely to survive.\n",
    "\n",
    "We are now finally ready to train our machine learning model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and evaluate a model\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning model is expected to not just explain known data, but make accurate predictions on new, unseen data. To assess the accuracy of a model, we must not test it on the same data we have trained it on because the model will then likely overfit: it will perform well on the training data, but will not generalize to new data. \n",
    "\n",
    "Instead, we will split our data set into two (random) subsets:\n",
    "\n",
    "- A **training set** to train our model on (typically 80% of the data).\n",
    "- A **testing set** (mutually exclusive from the training set) to validate our model on unseen data (typically 20% of the data).\n",
    "\n",
    "During training, the algorithm looks for patterns in the *training set* that link the features of each passenger to their survival. Following training, the model is used to predict survival of the passengers from the *test set*. By comparing the model predictions to the `Survived` column of the test set, we can assess the accuracy of our model.\n",
    "\n",
    "We split the data randomly using the method \"train_test_split\" from the *scikit-learn* library. We specify what fraction of the data should be used for the test set using the parameter `test_size`, and set a random seed using the parameter `random_state` so that our results will be reproducible. We then separate the `Survived` column, which contains the labels or the outcome, from the rest of the features in the data set: \n",
    "\n",
    "<div align=\"center\">\n",
    "<img width=\"800\" title=\"Splitting the data into training and test sets\" src=\"images/train_test_split.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data randomly into training set and test set in a 80:20 ratio\n",
    "train_df, test_df = train_test_split(titanic_data_clean, test_size=0.2, random_state=0)\n",
    "\n",
    "# Separate the `Survived` feature from the rest of the features\n",
    "X_train = train_df.drop(\"Survived\", axis=1)\n",
    "y_train = train_df[\"Survived\"]\n",
    "X_test = test_df.drop(\"Survived\", axis=1)\n",
    "y_test = test_df[\"Survived\"]\n",
    "\n",
    "# Verify dimensions of the four subsets \n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can select and train a model. The good news is that thanks to all the previous steps, things are going to be much simpler than you might think. The model we will use is called *Logistic Regression*, which is often the first model you will train when performing classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is commonly used to estimate the probability that an instance belongs to a particular class. If the estimated probability is greater than 50%, then the model predicts that the instance belongs to that class, and otherwise it predicts that it does not. This makes it a binary classifier. \n",
    "\n",
    "We will again be using the *scikit-learn* library. Each model in scikit-learn is implemented as a separate class, which we import and of which we create an instance--this instance is the model that we then train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression object\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Train the model on the training set\n",
    "logreg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! We now have a working Logistic Regression model. \n",
    "\n",
    "Let's use it to generate predictions on the test set. We can then calculate the accuracy of the model, i.e. the percentage of passengers correctly classified, by comparing the predicted values to the true values `y_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict `Survived' feature for the test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compare predicted values to true values\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = round(accuracy_score(y_test, y_pred) * 100, 3)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model achieved an accuracy of **80.5%** when tested against the test set. \n",
    "\n",
    "With the Logistic Regression model, we can also investigate the relative importance of the features contributing to its prediction of survival. Below, we display the coefficients associated with each feature in its decision function: Positive coefficients increase the log-odds of the response (and thus increase the probability for `Survived`=1), and negative coefficients decrease the log-odds of the response (and thus decrease the probability for `Survived`=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the coefficients of the features in the decision function.\n",
    "coeff_df = pd.DataFrame(train_df.columns.delete(0))\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n",
    "coeff_df.sort_values(by='Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients found during training confirm our initial observations:\n",
    "\n",
    "- `Sex` is associated with the highest positive coefficient, implying that as the Sex value increases (from `male`=0 to `female`=1), the probability of `Survived`=1 increases the most.\n",
    "- `Pclass` is associated with the highest negative coefficient, implying that as class increases (from 1 to 3), the probability of `Survived`=1 decreases the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  <b>Bonus Question 4</b>:    \n",
    "\n",
    "The code below repeats the splitting of the data, the training of the model, and its evaluation using the test set 1000 times. The accuracies obtained are displayed as a histogram. Can you explain why we get different accuracy values despite running the same code? What would be therefore a useful piece of information when evaluating the accuracy of a model? </div>\n",
    "\n",
    "<mark> Check performance on HUB! </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracies = []\n",
    "\n",
    "# Split data set randomly 1000 times, and train and test the model\n",
    "for repetition in range(1000):\n",
    "\n",
    "    # Split the data and separate the `Survived` feature\n",
    "    train_df, test_df = train_test_split(titanic_data_clean, test_size=0.2)\n",
    "    X_train = train_df.drop(\"Survived\", axis=1)\n",
    "    y_train = train_df[\"Survived\"]\n",
    "    X_test = test_df.drop(\"Survived\", axis=1)\n",
    "    y_test = test_df[\"Survived\"]\n",
    "\n",
    "    # Train and test the model\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train);\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    accuracy = round(accuracy_score(y_test, y_pred) * 100, 3)\n",
    "    \n",
    "    # Save accuracy value\n",
    "    model_accuracies.append(accuracy)\n",
    "\n",
    "# Display the accuracy distribution obtained    \n",
    "plt.hist(model_accuracies)\n",
    "plt.xlabel('model accuracy')\n",
    "plt.ylabel('number of repetitions')\n",
    "left, right = plt.xlim() \n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You may now wonder...\n",
    "\n",
    "...what things could we do to improve the accuracy of our model? Here are a few ideas:\n",
    "\n",
    "- Improving the features\n",
    "     - create new features from the existing data (feature engineering)\n",
    "     - experiment with preprocessing of data, e.g. use different methods to fill in missing values\n",
    "     - try to use information from features not considered here\n",
    "- Improving the model\n",
    "     - try a variety of models\n",
    "     - optimize the settings within each particular model (hyperparameter optimization)\n",
    "     - combine several models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congratulations! You have completed the Titanic Challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Based on the famous [kaggle Titanic competition](https://www.kaggle.com/c/titanic).\n",
    "\n",
    "Sources for pictures:\n",
    "- Titanic.jpg: https://upload.wikimedia.org/wikipedia/commons/9/92/Titanic.jpg\n",
    "\n",
    "### Python packages used\n",
    "\n",
    "This notebook uses several standard Python packages. These are:\n",
    "\n",
    "* **pandas** is a powerful data analysis package, providing the \"DataFrame\" structure to store data in memory and work with it easily and efficiently. DataFrame is a 2-dimensional labeled data structure with columns of potentially different types; you can think of it like a spreadsheet.\n",
    "* **numpy** is an essential package for scientific computing with Python, providing a fast numerical array structure and helper functions.\n",
    "* **random** generates pseudo-random numbers for various distributions.\n",
    "* **matplotlib** is the basic plotting library in Python; most other Python plotting libraries are built on top of it.\n",
    "* **seaborn** is an advanced statistical plotting library.\n",
    "* **sklearn** (**scikit-learn**) is an essential Machine Learning package in Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
